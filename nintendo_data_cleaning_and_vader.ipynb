{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/laurashummonmaass/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/laurashummonmaass/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/laurashummonmaass/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import datetime\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nintendo.trend_radar_functions import (json_to_df, \n",
    "                                            add_time_to_df,\n",
    "                                            reset_index,\n",
    "                                           )\n",
    "\n",
    "from nintendo.data_cleaning import (select_relevant_cols,\n",
    "                                    filter_lang,\n",
    "                                    drop_duplicates,\n",
    "                                    unique_hashtag_list,\n",
    "                                    unique_link_list,\n",
    "                                    unique_ats_list,\n",
    "                                    remove_hash_link_at,\n",
    "                                    strip_punctuation,\n",
    "                                    remove_punctuation,\n",
    "                                    make_lower_case,\n",
    "                                    get_wordnet_pos,\n",
    "                                    lemmatize_text,\n",
    "                                    remove_stop_words,\n",
    "                                    remove_just_hash,\n",
    "                                    vader_sentiment,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tweet Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please see Readme for instructions on how to acquire this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open('NintendoTweets.json', 'r'):\n",
    "    if len(line)>1:\n",
    "        tweets.append(json.loads(line))\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104695"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten JSON File for embedded dictionaries and store as DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_norm = json_to_df(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for English only (also uses only relevant columns)  \n",
    "Add a .time. column showing H:M:S    \n",
    "Remove any duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_lang(tweets_norm)\n",
    "df = add_time_to_df(df)\n",
    "df = drop_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71378"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 71378 entries, 0 to 104694\n",
      "Data columns (total 6 columns):\n",
      "user.id         71378 non-null float64\n",
      "text            71378 non-null object\n",
      "lang            71378 non-null object\n",
      "created_at      71378 non-null object\n",
      "timestamp_ms    71378 non-null object\n",
      ".time.          71378 non-null object\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.id         0\n",
       "text            0\n",
       "lang            0\n",
       "created_at      0\n",
       "timestamp_ms    0\n",
       ".time.          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:00:24\n",
      "17:00:23\n"
     ]
    }
   ],
   "source": [
    "print(df['.time.'].min())\n",
    "print(df['.time.'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any words starting with: #, @, or http and put cleaned text into new 'text2' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_hash_link_at(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation & stop words, make text all lower case, lemmatize all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_lower_case(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stop_words(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove http again (some links may have had symbol infront of it and not been removed the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_just_hash(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reset_index(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_twitter_df2.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference... to import the pickled DF back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('cleaned_twitter_df2.pkl', 'rb') as f:\n",
    "#     df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_output = vader_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound': 0.1406},\n",
       " {'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'compound': 0.5229},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_output[0:6] # view first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle Vader Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vader_output.pkl', 'wb') as f:\n",
    "    pickle.dump(vader_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference... to import the pickled Vader Sentiment back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('vader_output.pkl', 'rb') as f:\n",
    "#     vader_output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Vader NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVERYTHING HERE ON IS CLEANED UP AND LOCATED IN 'final_notebook_trends_and_radars.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE EVERYTHING BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Words DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_words = []\n",
    "# for i in df['text2']:\n",
    "#     words = i.split()\n",
    "#     for j in words:\n",
    "#         total_words.append(j)\n",
    "\n",
    "# unique_words = [] \n",
    "# for i in total_words:\n",
    "#     if not i in unique_words:\n",
    "#         unique_words.append(i)\n",
    "# unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(total_words))\n",
    "# print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(vocabulary=unique_words)\n",
    "# vectorized_words = vectorizer.transform(df['text2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorized_words.shape) # (tweet_count, unique_word_count)\n",
    "# word_array = vectorized_words.toarray()\n",
    "# word_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add matrix to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #vectorized_words is a matrix (71378, 14653)\n",
    "# #unique_words is a list (14653)\n",
    "# #df.index has len of (71378)\n",
    "\n",
    "# matrix_df = pd.DataFrame(word_array, columns=unique_words, index=df.index) \n",
    "# matrix_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns in df (text, lang, time) so that joining matrix has not conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.rename(index=str, columns={\"text\": \".text.\", \"lang\": \".lang.\", \"time\": \".time.\"})\n",
    "# df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Matrix DF and Original DF  \n",
    "* Need to remove original index and add new index column in both DF and Matrix_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('index', 1)\n",
    "# df = df.reset_index()\n",
    "# df = df.rename(index=str, columns={'index': 'df_index'})\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_df = matrix_df.reset_index()\n",
    "# matrix_df = matrix_df.rename(index=str, columns={'index': 'matrix_df_index'})\n",
    "# matrix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words to original df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_words = df.join(matrix_df)\n",
    "# df_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Create Words DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Lines\n",
    "## Grouped by every 5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Positive & Negative (& Compound) Scores by Second (for time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time \n",
    "    \n",
    "# df['.time.'] = df['timestamp_ms'].apply(lambda x: time.strftime('%H:%M:%S', time.gmtime(int(x)/1000)))\n",
    "# df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_seconds = []\n",
    "# for times in df['.time.']:\n",
    "#     all_times = []\n",
    "#     all_times.append(times)\n",
    "#     for i in all_times:\n",
    "#         if not i in unique_seconds:\n",
    "#             unique_seconds.append(i)\n",
    "# unique_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a column for each 5 second interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_seconds = []\n",
    "# for second in unique_seconds:\n",
    "#     if len(five_seconds)==0:\n",
    "#         five_seconds.append(1)\n",
    "#     elif len(five_seconds)%5 != 0:\n",
    "#         five_seconds.append(five_seconds[-1])\n",
    "#     else:\n",
    "#         five_seconds.append(five_seconds[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds_dict = dict(zip(unique_seconds, five_seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['five_seconds'] = df['.time.'].map(seconds_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend for sum of 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_sum_df = df.groupby('five_seconds').sum()\n",
    "# five_sum_df = five_sum_df.reset_index()\n",
    "# five_sum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(five_sum_df['five_seconds'], five_sum_df['pos'], color='g')\n",
    "# plt.plot(five_sum_df['five_seconds'], five_sum_df['neg'], color='orange')\n",
    "# plt.xlabel('Every 5 Seconds')\n",
    "# plt.ylabel('Sentiment')\n",
    "# plt.title('Nintendo E3 Twitter Sentiments')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend for mean of 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_mean_df = df.groupby('five_seconds').mean()\n",
    "# five_mean_df = five_mean_df.reset_index()\n",
    "# five_mean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(five_mean_df['five_seconds'], five_mean_df['pos'], color='g')\n",
    "# plt.plot(five_mean_df['five_seconds'], five_mean_df['neg'], color='orange')\n",
    "# plt.xlabel('Every 5 Seconds')\n",
    "# plt.ylabel('Sentiment')\n",
    "# plt.title('Nintendo E3 Twitter Sentiments')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Trend Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major Announcements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Major Announcement Points & the Specific Words that Occured the Most\n",
    "May end up not using and using only 5 second intervals instead... can label 5 second intervals as specific topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['five_seconds'] == 81] .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mario_party = list(range(81,101))     #81 to 100\n",
    "# smash_brothers = list(range(210,517)) #210 to 516\n",
    "# end = list(range(517, 601))           #517 to 600\n",
    "# #none for all others\n",
    "\n",
    "# df['five_seconds'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_five_seconds = list(range(1, 721))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = []\n",
    "# for i in all_five_seconds:\n",
    "#     if i in mario_party:\n",
    "#         labels.append('mario_party')\n",
    "#     elif i in smash_brothers:\n",
    "#         labels.append('smash_brothers')\n",
    "#     elif i in end:\n",
    "#         labels.append('end')\n",
    "#     else:\n",
    "#         labels.append('none')\n",
    "# len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary that maps the labels to the appropriate 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_dict = dict(zip(all_five_seconds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['.announcements.'] = df['five_seconds'].map(labels_dict)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Major Announcements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: May not use... may instead create a radar plot for each SECOND and create a general label for all seconds in the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_words.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_words['five_seconds'] = df_words['.time.'].map(seconds_dict)\n",
    "# df_words['.announcements.'] = df_words['five_seconds'].map(labels_dict)\n",
    "\n",
    "# def create_dictionary_for_specified_time (time=1, which_five='top'): # choose either 'top' or 'bottom'\n",
    "#     df_filtered_by_seconds = df_words.loc[(df_words['five_seconds']== time)]  #| (df_words['five_seconds']== 2)]\n",
    "#     dict_by_seconds = df_filtered_by_seconds.to_dict(orient='index')\n",
    "    \n",
    "#     # create a cleaned dictionary for each word labeled by tweet number\n",
    "#     list_of_word_dicts = []\n",
    "#     for key1, val in dict_by_seconds.items():\n",
    "#         u_words = val['text2'].split(' ')\n",
    "#         neg = val['neg']\n",
    "#         compound = val['compound']\n",
    "#         neu = val['neu']\n",
    "#         pos = val['pos']\n",
    "#         for key, value in val.items():\n",
    "#             try:\n",
    "#                 value = float(value)\n",
    "#                 if (value > 0) & (key in u_words) :\n",
    "#                     list_of_word_dicts.append({ \n",
    "#                             'tweet_no': key1,\n",
    "#                             key:{'count': 1, 'compound_sum': compound, 'neg_sum': neg, \n",
    "#                                  'neu_sum': neu, 'pos_sum': pos},\n",
    "#                                                 })\n",
    "#             except:\n",
    "#                 pass\n",
    "    \n",
    "#     # remove duplicate words that appear several times in one tweet\n",
    "#     no_dupl_list_of_word_dicts = [i for n, i in enumerate(list_of_word_dicts) \n",
    "#                                   if i not in list_of_word_dicts[n + 1:]]\n",
    "    \n",
    "#     return_dict = {}\n",
    "#     for i in no_dupl_list_of_word_dicts:\n",
    "#         for key, val in i.items():\n",
    "#             if key is not 'tweet_no':\n",
    "#                 if key not in return_dict.keys():\n",
    "#                     return_dict.update({key : val})\n",
    "#                 else:\n",
    "#                     return_dict[key]['count'] += val['count']\n",
    "#                     return_dict[key]['compound_sum'] += val['compound_sum']\n",
    "#                     return_dict[key]['neg_sum'] += val['neg_sum']\n",
    "#                     return_dict[key]['neu_sum'] += val['neu_sum']\n",
    "#                     return_dict[key]['pos_sum'] += val['pos_sum']\n",
    "                    \n",
    "#     compound_dict = {}\n",
    "#     for key, val in return_dict.items():\n",
    "#         #print(key, val)\n",
    "#         #compound_dict.update({key: val['compound_sum'] })\n",
    "#         compound_dict[key] = val['compound_sum']\n",
    "    \n",
    "#     sorted_compound_dict = sorted(compound_dict.items(), key=lambda kv: kv[1])\n",
    "    \n",
    "#     if which_five == 'top':\n",
    "#         #five_words = dict(sorted_compound_dict[0:5])\n",
    "#         five_words = dict(sorted_compound_dict[-5:])\n",
    "#     elif which_five == 'bottom': \n",
    "#         #five_words = dict(sorted_compound_dict[-5:])\n",
    "#         five_words = dict(sorted_compound_dict[0:5])\n",
    "#     else:\n",
    "#         \"Please choose either 'top' or 'bottom'.\"\n",
    "\n",
    "#     return five_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing = create_dictionary_for_specified_time(2)\n",
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom_df = pd.Series(testing[0])\n",
    "# bottom_df = pd.DataFrame(bottom_df)\n",
    "# bottom_df = bottom_df.T\n",
    "# bottom_df['group'] = 'A'\n",
    "\n",
    "# top_df = pd.Series(testing[1])\n",
    "# top_df = pd.DataFrame(top_df)\n",
    "# top_df = top_df.T\n",
    "# top_df['group'] = 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Word Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NOT WORKING YET\n",
    "# def radar_plot_creator():\n",
    "#     bottom_df = pd.Series(testing[0])\n",
    "#     bottom_df = pd.DataFrame(bottom_df)\n",
    "#     bottom_df = bottom_df.T\n",
    "#     bottom_df['group'] = 'A'\n",
    "#     top_df = pd.Series(testing[1])\n",
    "#     top_df = pd.DataFrame(top_df)\n",
    "#     top_df = top_df.T\n",
    "#     top_df['group'] = 'A'\n",
    "    \n",
    "#    # Set data\n",
    "#     radar_df_test = bottom_df\n",
    "\n",
    "#     # number of variable\n",
    "#     categories=list(radar_df_test)[1:]\n",
    "#     N = len(categories)\n",
    "\n",
    "#     # We are going to plot the first line of the data frame.\n",
    "#     # But we need to repeat the first value to close the circular graph:\n",
    "#     values=radar_df_test.loc[0].drop('group').values.flatten().tolist()\n",
    "#     values += values[:1]\n",
    "#     values\n",
    "\n",
    "#     # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "#     angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "#     angles += angles[:1]\n",
    "\n",
    "#     # Initialise the spider plot\n",
    "#     ax = plt.subplot(111, polar=True)\n",
    "\n",
    "#     # Draw one axe per variable + add labels labels yet\n",
    "#     plt.xticks(angles[:-1], categories, color='grey', size=8)\n",
    "\n",
    "#     # Draw ylabels\n",
    "#     ax.set_rlabel_position(0)\n",
    "#     plt.yticks([-3,-2,-1,0,1,2,3], [\"\",\"\",\"\", 0, \"\", \"\", \"\"], color=\"grey\", size=7)\n",
    "#     plt.ylim(-3,3)\n",
    "\n",
    "#     # Plot data\n",
    "#     ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "\n",
    "#     # Fill area\n",
    "#     testing_radar = ax.fill(angles, values, 'b', alpha=0.1);  \n",
    "    \n",
    "#     return testing_radar\n",
    "# radar_plot_creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS ONE WORKS.... But not in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set data\n",
    "# radar_df_test = bottom_df\n",
    " \n",
    "# # number of variable\n",
    "# categories=list(radar_df_test)[1:]\n",
    "# N = len(categories)\n",
    " \n",
    "# # We are going to plot the first line of the data frame.\n",
    "# # But we need to repeat the first value to close the circular graph:\n",
    "# values=radar_df_test.loc[0].drop('group').values.flatten().tolist()\n",
    "# values += values[:1]\n",
    "# values\n",
    " \n",
    "# # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "# angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "# angles += angles[:1]\n",
    " \n",
    "# # Initialise the spider plot\n",
    "# ax = plt.subplot(111, polar=True)\n",
    " \n",
    "# # Draw one axe per variable + add labels labels yet\n",
    "# plt.xticks(angles[:-1], categories, color='grey', size=8)\n",
    " \n",
    "# # Draw ylabels\n",
    "# ax.set_rlabel_position(0)\n",
    "# plt.yticks([-3,-2,-1,0,1,2,3], [\"\",\"\",\"\", 0, \"\", \"\", \"\"], color=\"grey\", size=7)\n",
    "# plt.ylim(-3,3)\n",
    " \n",
    "# # Plot data\n",
    "# ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    " \n",
    "# # Fill area\n",
    "# ax.fill(angles, values, 'b', alpha=0.1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Radar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
